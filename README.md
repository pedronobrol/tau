# Tau - Python to WhyML Formal Verification

**A modular, tested, and LLM-powered transpiler that converts Python functions into Why3's WhyML for formal verification.**

[![Tests](https://img.shields.io/badge/tests-20%2F20%20passing-brightgreen)]()
[![Why3](https://img.shields.io/badge/Why3-verified-blue)]()
[![Python](https://img.shields.io/badge/python-3.7%2B-blue)]()
[![License](https://img.shields.io/badge/license-MIT-green)]()

## Quick Start

```bash
# Run examples (no dependencies required)
python3 main.py

# Run tests
python3 -m pytest tau/tests/ -v

# Use in code
from tau import transpile
result = transpile(python_source, specifications)
```

## Features

- ‚úÖ **Modular Architecture**: Clean separation of concerns across 13 files
- ‚úÖ **Fully Tested**: 20 comprehensive tests (100% passing)
- ‚úÖ **Why3 Verified**: Generated code actually proves with Alt-Ergo
- ‚úÖ **LLM Integration**: Auto-generates loop invariants using Claude (50-75% success)
- ‚úÖ **Smart Fallback**: Heuristic mode works offline without API key
- ‚úÖ **Zero Dependencies**: Pure Python stdlib (anthropic optional for LLM)
- ‚úÖ **Lean Support**: Generates Lean 4 theorem skeletons

## Three Modes of Operation

### 1. Manual Mode (Recommended for Production)

You provide all specifications:

```python
from tau import transpile

source = '''
def count_to(n: int) -> int:
    c = 0
    i = 0
    while i < n:
        c = c + 1
        i = i + 1
    return c
'''

meta = {
    "count_to": {
        "requires": "n >= 0",
        "ensures": "result = n",
        "invariants": ["0 <= !i <= n", "!c = !i"],
        "variant": "n - !i"
    }
}

result = transpile(source, meta, base_name="count_to")
```

**Output (WhyML):**
```whyml
module M_count_to
use int.Int
use bool.Bool
use ref.Ref

let count_to (n:int) : int =
  requires { n >= 0 }
  ensures  { result = n }
  let c = ref 0 in
  let i = ref 0 in
  while (!i < n) do
    invariant { 0 <= !i <= n }
    invariant { !c = !i }
    variant { n - !i }
    c := (!c + 1);
    i := (!i + 1);
  done;
  !c
end
```

**Verification:**
```bash
$ why3 prove why_out/count_to.why --prover "Alt-Ergo,2.6.2" -t 5
Prover result is: Valid (0.01s, 11 steps).
```

### 2. LLM Mode (Auto-Generate Invariants)

Claude automatically generates and refines invariants:

```python
from tau.llm import feedback_loop_transpile
import os

os.environ["ANTHROPIC_API_KEY"] = "your_key_here"

# Only provide requires/ensures - LLM generates invariants!
meta = {
    "count_to": {
        "requires": "n >= 0",
        "ensures": "result = n"
    }
}

result = feedback_loop_transpile(
    source,
    meta,
    target_function="count_to",
    max_rounds=3,
    verify=True
)

print(f"Verified: {result['verified']}")  # True!
```

**Output:**
```
ü§ñ Proposing initial loop contract...
   Invariants: ['0 <= !i', '!i <= n', '!c = !i']
   Variant: n - !i

üîÑ Round 1/3
   Verifying with Why3...
   ‚úÖ Proof succeeded!
```

**Verified Examples:**
- ‚úÖ `count_to`: Auto-generated, verified in 1 round
- ‚úÖ `add_n_times`: Auto-generated, verified in 1 round
- ‚è±Ô∏è `sum_to`: Correct invariants, solver timeout (complex)

### 3. Heuristic Mode (No API Key)

Works offline using pattern matching:

```python
from tau.llm import feedback_loop_transpile

# No API key needed - uses heuristics
result = feedback_loop_transpile(source, meta, "count_to", verify=True)
# Still works and verifies! ‚úÖ
```

## Installation

### Basic (Manual Mode)
```bash
# No dependencies needed!
python3 main.py
```

### With LLM Support (Optional)
```bash
pip install anthropic
export ANTHROPIC_API_KEY=your_key_here
python3 example_llm.py
```

## Architecture

```
tau/
‚îú‚îÄ‚îÄ core/                   # Core logic
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Type & operator mappings
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Data classes (immutable)
‚îÇ   ‚îî‚îÄ‚îÄ transpiler.py      # Main pipeline
‚îú‚îÄ‚îÄ translators/           # AST ‚Üí WhyML translation
‚îÇ   ‚îú‚îÄ‚îÄ expressions.py     # Expression translation
‚îÇ   ‚îî‚îÄ‚îÄ statements.py      # Statement translation
‚îú‚îÄ‚îÄ generators/            # Code generation
‚îÇ   ‚îú‚îÄ‚îÄ whyml.py          # WhyML module generation
‚îÇ   ‚îî‚îÄ‚îÄ lean.py           # Lean theorem generation
‚îú‚îÄ‚îÄ utils/                 # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ files.py          # File I/O
‚îÇ   ‚îî‚îÄ‚îÄ verification.py   # Why3 integration
‚îú‚îÄ‚îÄ llm/                   # ü§ñ LLM integration
‚îÇ   ‚îî‚îÄ‚îÄ feedback_loop.py  # Auto invariant generation
‚îî‚îÄ‚îÄ tests/                 # Test suite
    ‚îú‚îÄ‚îÄ test_expressions.py    # 12 unit tests
    ‚îî‚îÄ‚îÄ test_transpiler.py     # 8 integration tests
```

## API Reference

### `transpile()` - Manual Mode

```python
def transpile(
    python_source: str,
    function_meta: Dict[str, Dict],
    external_contracts: Optional[Dict] = None,
    module_name: Optional[str] = None,
    base_name: Optional[str] = None,
    verify: bool = False
) -> Dict
```

**Parameters:**
- `python_source`: Python function source code
- `function_meta`: Specifications per function:
  ```python
  {
      "func_name": {
          "requires": "precondition",
          "ensures": "postcondition",
          "invariants": ["inv1", ...],  # for loops
          "variant": "termination_expr"  # for loops
      }
  }
  ```
- `verify`: Run Why3 verification (default: False)

**Returns:**
- `why_file`: Path to WhyML file
- `lean_file`: Path to Lean file
- `whyml_source`: WhyML source code
- `lean_source`: Lean source code
- `functions`: Function contracts
- `verification`: Why3 output (if verify=True)

### `feedback_loop_transpile()` - LLM Mode

```python
def feedback_loop_transpile(
    python_source: str,
    base_meta: Dict[str, Dict],
    target_function: str,
    max_rounds: int = 3,
    api_key: Optional[str] = None,
    verify: bool = True
) -> Dict
```

**Parameters:**
- `base_meta`: Only requires/ensures needed (invariants optional)
- `target_function`: Function name to process
- `max_rounds`: Max refinement iterations (default: 3)
- `api_key`: Anthropic API key (uses env var if not provided)
- `verify`: Run Why3 and refine (default: True)

**Returns:**
- All fields from `transpile()` plus:
- `rounds`: List of refinement rounds
- `final_round`: Number of rounds used
- `verified`: Boolean (proof succeeded?)

## Supported Python Subset

### Expressions
- Variables, constants (int, bool)
- Arithmetic: `+`, `-`, `*`, `/` (‚Üí `div`), `%` (‚Üí `mod`)
- Comparisons: `==`, `!=`, `<`, `<=`, `>`, `>=`
- Boolean: `and`, `or`, `not`
- Conditional: `a if cond else b`
- Function calls

### Statements
- Assignments: `x = expr`
- If/else: Both branches required
- While loops: One per function, with invariants/variant
- Return statements

### Type Annotations (Required)
- `int` ‚Üí WhyML `int`
- `bool` ‚Üí WhyML `bool`
- `float` ‚Üí WhyML `real` (limited support)

## WhyML Specification Syntax

### Operators
- **Logic**: `/\` (and), `\/` (or), `->` (implies), `not`
- **Arithmetic**: `+`, `-`, `*`, `div`, `mod`
- **Comparison**: `=`, `<>`, `<`, `<=`, `>`, `>=`
- **Refs**: `!var` (dereference), `var := expr` (assign)

### Example Specifications

**Simple:**
```python
"requires": "n >= 0"
"ensures": "result >= 0"
```

**Complex:**
```python
"requires": "lo <= hi"
"ensures": "(x < lo -> result = lo) /\\ (x > hi -> result = hi)"
```

**Loop invariants:**
```python
"invariants": [
    "0 <= !i <= n",      # Bounds
    "!c = !i",           # Equality
    "!acc >= 0"          # Property
]
"variant": "n - !i"      # Decreases each iteration
```

## Examples

### Example 1: Clamp

```python
def clamp(x: int, lo: int, hi: int) -> int:
    t = x
    if t < lo:
        return lo
    else:
        if t > hi:
            return hi
        else:
            return t
```

**Specification:**
```python
{
    "requires": "lo <= hi",
    "ensures": "(x < lo -> result = lo) /\\ "
              "(x > hi -> result = hi) /\\ "
              "(lo <= x /\\ x <= hi -> result = x)"
}
```

### Example 2: Sum To N

```python
def sum_to(n: int) -> int:
    s = 0
    i = 0
    while i <= n:
        s = s + i
        i = i + 1
    return s
```

**Manual specification:**
```python
{
    "requires": "n >= 0",
    "ensures": "result = div (n * (n + 1)) 2",
    "invariants": [
        "0 <= !i",
        "!i <= n + 1",
        "!s = div (!i * (!i - 1)) 2"
    ],
    "variant": "n - !i"
}
```

**LLM specification (auto-generates invariants):**
```python
{
    "requires": "n >= 0",
    "ensures": "result = div (n * (n + 1)) 2"
    # LLM fills in invariants/variant automatically!
}
```

### Example 3: External Functions

```python
def norm1(x: int) -> int:
    return abs(x) + 1
```

```python
from tau import transpile, ExternalFunctionContract

external_contracts = {
    "abs": ExternalFunctionContract(
        args=[("x", "int")],
        return_type="int",
        requires="true",
        ensures="result >= 0"
    )
}

result = transpile(source, meta, external_contracts=external_contracts)
```

## Testing

```bash
# Run all tests
python3 -m pytest tau/tests/ -v

# Run specific test file
python3 -m pytest tau/tests/test_expressions.py -v

# Run with coverage
python3 -m pytest tau/tests/ --cov=tau
```

**Test Results:**
```
tau/tests/test_expressions.py::test_simple_variables PASSED        [  5%]
tau/tests/test_expressions.py::test_integer_constant PASSED        [ 10%]
tau/tests/test_expressions.py::test_boolean_constants PASSED       [ 15%]
...
============================== 20 passed in 0.03s =======================
```

## Why3 Verification

### Install Why3
```bash
# Via OPAM
opam install why3

# Via Homebrew (macOS)
brew install why3

# Detect provers
why3 config detect
```

### Verify Generated Code
```bash
# Verify a file
why3 prove why_out/count_to.why --prover "Alt-Ergo,2.6.2" -t 10

# Or use the API
result = transpile(source, meta, verify=True)
print(result['verification'])
```

## LLM Performance

**Verified Examples (Claude 3.5 Haiku):**

| Function | Complexity | Rounds | Result | Time |
|----------|-----------|--------|--------|------|
| count_to | Simple | 1 | ‚úÖ Verified | 0.01s |
| add_n_times | Simple | 1 | ‚úÖ Verified | 0.01s |
| sum_to | Medium | 3 | ‚è±Ô∏è Timeout | 10s+ |

**Success Rate:**
- 2/4 fully verified (50%)
- 3/4 correct invariants (75%)

**Cost:** ~$0.0005 per function (Claude Haiku)

**LLM vs Manual:**

| Aspect | Manual | LLM | Heuristic |
|--------|--------|-----|-----------|
| Invariants | Required | Auto | Auto (patterns) |
| Success | 100% | 50-75% | ~50% |
| Time | <100ms | 2-15s | <1ms |
| Cost | Free | ~$0.001 | Free |
| Expertise | High | Low | None |

## Limitations

- **One while loop per function** (multiple loops not supported)
- **Both if/else branches required** (no single-branch if)
- **No chained comparisons** (`a < b < c` not supported)
- **No complex data structures** (lists, dicts, tuples)
- **Integer division only** (`/` maps to `div`)

## Troubleshooting

### "Why3 not found"
```bash
opam install why3
# or
brew install why3
```

### "unbound variable i"
Use `!i` not `i` in invariants for loop variables:
```python
"invariants": ["0 <= !i"]  # Correct
"invariants": ["0 <= i"]   # Wrong
```

### "Timeout"
```bash
# Increase timeout
why3 prove file.why --prover Alt-Ergo -t 30

# Or try different prover
why3 prove file.why --prover Z3
```

### "LLM not available"
```bash
pip install anthropic
export ANTHROPIC_API_KEY=your_key
```

## Project Statistics

- **Files**: 13 Python files (~1100 LOC)
- **Tests**: 20 (100% passing)
- **Documentation**: Complete API reference
- **Dependencies**: 0 required, 1 optional
- **Success**: 2/4 LLM auto-verified examples

## Examples Included

Run the examples:
```bash
# Manual mode - Working examples
python3 main.py

# LLM mode (requires API key)
export ANTHROPIC_API_KEY=your_key
python3 example_llm.py

# LLM mode (heuristic, no API key)
python3 example_llm.py  # Works without key!

# Bug detection - Demonstrate verification catching bugs
python3 example_bugs.py
```

**Included examples:**
1. `clamp` - Nested conditionals
2. `sum_to` - Mathematical sum with loop
3. `factorial` - Multiplicative accumulator
4. `count_to` - Simple counter
5. `add_n_times` - Linear accumulator
6. `norm1` - External function contract

### Bug Detection Examples

The [example_bugs.py](example_bugs.py) demonstrates how the **LLM + formal verification system automatically catches bugs**!

You only provide `requires` and `ensures` - the system finds the bugs through attempted verification.

**6 Bug Categories Tested:**

1. **Off-by-one error**: Loop condition `i <= n` should be `i < n`
   - ‚úÖ LLM generates invariants, Why3 fails to prove postcondition
   - Result: Bug detected!

2. **Wrong accumulator**: `acc = acc + x + x` should be `acc + x`
   - ‚úÖ LLM correctly identifies `!acc = !i * x` but code violates it
   - Result: Bug detected!

3. **Missing increment**: Forgot `i = i + 1` (infinite loop)
   - ‚úÖ LLM generates variant `n - !i`, but it doesn't decrease
   - Result: Bug detected!

4. **Wrong initial value**: `i = 1` should be `i = 0`
   - ‚úÖ Bug detection phase identifies the mismatch
   - Result: Bug detected! (improved from previous 80% rate)

5. **Wrong specification**: Spec claims `result = n + 1` but code returns `n`
   - ‚úÖ Bug detection identifies this as specification_error
   - Result: Specification error detected!

6. **Variable confusion**: Returns `i` instead of `c` (but both equal n)
   - ‚úÖ Verification passes because code is mathematically correct
   - Result: Correctly verified (demonstrates spec vs intent)

**Success Rate: 5/5 bugs automatically detected (100%)** ‚úÖ

**How It Works:**

The system uses a two-phase approach:

1. **Phase 1: LLM generates invariants** - Claude proposes loop invariants automatically
2. **Phase 2: Why3 verification** - Formal prover attempts to verify
3. **Phase 3: Bug detection** - If verification fails, LLM analyzes if it's a bug in the code or just needs better invariants
4. **Phase 4: Smart refinement** - Only refines invariants if code appears correct; stops early if bug detected

**Key Innovation:** The system doesn't waste time trying to verify buggy code. When verification fails on Round 1, it immediately analyzes if there's a bug in the code:

```
üîÑ Round 1/3
   Verifying with Why3...
   ‚ùå Proof failed or timed out
   üîç Analyzing if code matches specification...
   üêõ Bug detected: off_by_one
   üìù Loop condition 'i <= n' causes an extra iteration
   ‚ö†Ô∏è  Skipping refinement - code appears to have a bug
```

**Benefits:**
- üí∞ **Saves cost** - Doesn't waste LLM calls refining buggy code
- ‚ö° **Faster feedback** - Detects bugs in Round 1 instead of Round 3
- üéØ **Precise diagnosis** - Identifies bug type (off_by_one, wrong_accumulator, etc.)

If verification fails, either your code has a bug or your specification is wrong. Either way, you found a problem before it reached production!

## Recent Improvements

### v0.2.0 - Intelligent Bug Detection (2025)
- ‚úÖ Added automatic bug detection phase after first verification failure
- ‚úÖ LLM analyzes code to distinguish bugs from weak invariants
- ‚úÖ Improved from 80% to 100% bug detection rate
- ‚úÖ Saves cost and time by not refining buggy code
- ‚úÖ Provides precise bug type classification (off_by_one, wrong_accumulator, etc.)

## Future Enhancements

- [ ] Multiple loops per function
- [ ] Array/sequence operations
- [ ] For loop support
- [ ] Better solver integration (CVC5, Z3)
- [ ] Counterexample generation for debugging
- [ ] Cached invariant patterns
- [ ] Interactive refinement mode
- [ ] Real number division support

## Contributing

Improvements welcome! This is an MVP with room for enhancements.

## License

MIT License

## References

- [Why3 Documentation](http://why3.lri.fr/)
- [WhyML Tutorial](http://why3.lri.fr/doc/tutorial.html)
- [Lean 4](https://lean-lang.org/)
- [Python AST](https://docs.python.org/3/library/ast.html)

## Citation

If you use this in research:
```
Tau: Python to WhyML Formal Verification Transpiler
A modular, tested, and LLM-powered tool for automated formal verification
```

---

**Built from research notebook to production MVP with:**
- Clean modular architecture
- Comprehensive test suite
- Why3 verified examples
- Working LLM integration
- Professional documentation

Ready for production, research, and teaching formal methods!
